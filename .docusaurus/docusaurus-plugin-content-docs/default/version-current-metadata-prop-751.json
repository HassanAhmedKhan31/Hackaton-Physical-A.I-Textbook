{
  "pluginId": "default",
  "version": "current",
  "label": "Next",
  "banner": null,
  "badge": false,
  "noIndex": false,
  "className": "docs-version-current",
  "isLast": true,
  "docsSidebars": {
    "tutorialSidebar": [
      {
        "type": "category",
        "label": "Module 1: The Robotic Nervous System",
        "items": [
          {
            "type": "link",
            "label": "The Nervous System of the Robot",
            "href": "/module-1/intro-nervous-system",
            "docId": "module-1/intro-nervous-system",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Neurons & Nodes",
            "href": "/module-1/nodes-and-rclpy",
            "docId": "module-1/nodes-and-rclpy",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Signals & Synapses (Topics)",
            "href": "/module-1/topics-and-messages",
            "docId": "module-1/topics-and-messages",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Reflexes & Requests (Services)",
            "href": "/module-1/services-actions",
            "docId": "module-1/services-actions",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "The Body Schema (URDF)",
            "href": "/module-1/urdf-humanoid-body",
            "docId": "module-1/urdf-humanoid-body",
            "unlisted": false
          }
        ],
        "collapsed": true,
        "collapsible": true
      },
      {
        "type": "category",
        "label": "Module 2: ROS 2 Fundamentals",
        "items": [
          {
            "type": "link",
            "label": "The Workspace (The Robot's Home)",
            "href": "/module-2/workspaces-and-packages",
            "docId": "module-2/workspaces-and-packages",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Launch Systems (Waking Up the Robot)",
            "href": "/module-2/launch-files",
            "docId": "module-2/launch-files",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Runtime Configuration (Parameters)",
            "href": "/module-2/parameters",
            "docId": "module-2/parameters",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Long-Running Tasks (Actions)",
            "href": "/module-2/actions",
            "docId": "module-2/actions",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Defining the Protocol (Messages)",
            "href": "/module-2/custom-interfaces",
            "docId": "module-2/custom-interfaces",
            "unlisted": false
          }
        ],
        "collapsed": true,
        "collapsible": true
      },
      {
        "type": "category",
        "label": "Module 3: Robot Simulation",
        "items": [
          {
            "type": "link",
            "label": "The Simulation Lab (Setting up Gazebo)",
            "href": "/module-3/gazebo-environment",
            "docId": "module-3/gazebo-environment",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "The Format War (URDF vs. SDF)",
            "href": "/module-3/urdf-vs-sdf",
            "docId": "module-3/urdf-vs-sdf",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Building the World",
            "href": "/module-3/physics-and-worlds",
            "docId": "module-3/physics-and-worlds",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Virtual Eyes & Ears (Sensors)",
            "href": "/module-3/simulating-sensors",
            "docId": "module-3/simulating-sensors",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "The Pretty Face (Unity Visualization)",
            "href": "/module-3/unity-visualization",
            "docId": "module-3/unity-visualization",
            "unlisted": false
          }
        ],
        "collapsed": true,
        "collapsible": true
      },
      {
        "type": "category",
        "label": "Module 4: NVIDIA Isaac AI",
        "items": [
          {
            "type": "link",
            "label": "The Omniverse (Isaac Sim)",
            "href": "/module-4/isaac-sim-omniverse",
            "docId": "module-4/isaac-sim-omniverse",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Supercharging ROS (Isaac ROS GEMs)",
            "href": "/module-4/isaac-ros-gems",
            "docId": "module-4/isaac-ros-gems",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Spatial Awareness (VSLAM & Nav2)",
            "href": "/module-4/nav2-and-vslam",
            "docId": "module-4/nav2-and-vslam",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Training the Brain (Synthetic Data)",
            "href": "/module-4/synthetic-data-generation",
            "docId": "module-4/synthetic-data-generation",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Walking with AI (Reinforcement Learning)",
            "href": "/module-4/rl-and-sim-to-real",
            "docId": "module-4/rl-and-sim-to-real",
            "unlisted": false
          }
        ],
        "collapsed": true,
        "collapsible": true
      },
      {
        "type": "category",
        "label": "Module 5: Humanoid Development",
        "items": [
          {
            "type": "link",
            "label": "The Math of Movement (Kinematics)",
            "href": "/module-5/kinematics-dynamics",
            "docId": "module-5/kinematics-dynamics",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "The Art of Falling (Bipedal Walking)",
            "href": "/module-5/bipedal-locomotion",
            "docId": "module-5/bipedal-locomotion",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Grasping the World (MoveIt 2)",
            "href": "/module-5/manipulation-grasping",
            "docId": "module-5/manipulation-grasping",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Social Robotics (HRI)",
            "href": "/module-5/human-robot-interaction",
            "docId": "module-5/human-robot-interaction",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Assembling the Avatar (The Capstone)",
            "href": "/module-5/capstone-integration",
            "docId": "module-5/capstone-integration",
            "unlisted": false
          }
        ],
        "collapsed": true,
        "collapsible": true
      },
      {
        "type": "category",
        "label": "Module 6: Conversational Robotics",
        "items": [
          {
            "type": "link",
            "label": "The Voice Interface (VUI)",
            "href": "/module-6/intro-conversational-robotics",
            "docId": "module-6/intro-conversational-robotics",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "The Robot's Ears (Speech Recognition)",
            "href": "/module-6/hearing-with-whisper",
            "docId": "module-6/hearing-with-whisper",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "The Cognitive Core (GPT Integration)",
            "href": "/module-6/thinking-with-llms",
            "docId": "module-6/thinking-with-llms",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Vision-Language-Action (VLA)",
            "href": "/module-6/multimodal-interaction",
            "docId": "module-6/multimodal-interaction",
            "unlisted": false
          },
          {
            "type": "link",
            "label": "Living with Robots (Ethics & Future)",
            "href": "/module-6/future-ethics",
            "docId": "module-6/future-ethics",
            "unlisted": false
          }
        ],
        "collapsed": true,
        "collapsible": true
      }
    ]
  },
  "docs": {
    "module-1/intro-nervous-system": {
      "id": "module-1/intro-nervous-system",
      "title": "The Nervous System of the Robot",
      "description": "<PersonalizedContent",
      "sidebar": "tutorialSidebar"
    },
    "module-1/nodes-and-rclpy": {
      "id": "module-1/nodes-and-rclpy",
      "title": "Neurons & Nodes",
      "description": "In the biological analogy, a Node is a neuron or a cluster of neurons dedicated to a single task. In ROS 2 terms, a Node is a single executable process—a Python script—that performs computation.",
      "sidebar": "tutorialSidebar"
    },
    "module-1/services-actions": {
      "id": "module-1/services-actions",
      "title": "Reflexes & Requests (Services)",
      "description": "Topics are great for continuous data (sensor streams, heartbeats), like listening to a radio. But sometimes you need to ask a specific question and get a specific answer.",
      "sidebar": "tutorialSidebar"
    },
    "module-1/topics-and-messages": {
      "id": "module-1/topics-and-messages",
      "title": "Signals & Synapses (Topics)",
      "description": "If Nodes are neurons, Topics are the synapses—the gaps where signals jump from one neuron to another.",
      "sidebar": "tutorialSidebar"
    },
    "module-1/urdf-humanoid-body": {
      "id": "module-1/urdf-humanoid-body",
      "title": "The Body Schema (URDF)",
      "description": "A nervous system is useless without a body. In ROS 2, the robot's physical structure is defined by the Unified Robot Description Format (URDF). This is an XML file that tells the robot:",
      "sidebar": "tutorialSidebar"
    },
    "module-2/actions": {
      "id": "module-2/actions",
      "title": "Long-Running Tasks (Actions)",
      "description": "In Module 1, we learned about Services (Request -> Response). Services are like a function call: you ask for something, and you wait until you get the answer. This works great for \"Turn on LED\" or \"Get Battery Level.\"",
      "sidebar": "tutorialSidebar"
    },
    "module-2/custom-interfaces": {
      "id": "module-2/custom-interfaces",
      "title": "Defining the Protocol (Messages)",
      "description": "In any language, you need words to communicate. If you want to talk about \"Philosophy,\" you need complex words. If you want to talk about \"Rocket Science,\" you need specific technical terms.",
      "sidebar": "tutorialSidebar"
    },
    "module-2/launch-files": {
      "id": "module-2/launch-files",
      "title": "Launch Systems (Waking Up the Robot)",
      "description": "A humanoid robot has many distinct parts: vision, balance, motor control, path planning, and voice recognition. If each of these is a separate Node (as they should be), starting them manually would be a nightmare. You would need to open 20 terminal tabs and type 20 commands every time you turned the robot on.",
      "sidebar": "tutorialSidebar"
    },
    "module-2/parameters": {
      "id": "module-2/parameters",
      "title": "Runtime Configuration (Parameters)",
      "description": "Hard-coding values is a bad habit in software engineering. If you hard-code your robot's walking speed to 1.0 m/s, you have to stop the robot, edit the code, rebuild, and restart just to test 0.5 m/s.",
      "sidebar": "tutorialSidebar"
    },
    "module-2/workspaces-and-packages": {
      "id": "module-2/workspaces-and-packages",
      "title": "The Workspace (The Robot's Home)",
      "description": "Before we start building the brain of our humanoid, we need a place to work. In ROS 2, this is called a Workspace.",
      "sidebar": "tutorialSidebar"
    },
    "module-3/gazebo-environment": {
      "id": "module-3/gazebo-environment",
      "title": "The Simulation Lab (Setting up Gazebo)",
      "description": "Welcome to the Matrix.",
      "sidebar": "tutorialSidebar"
    },
    "module-3/physics-and-worlds": {
      "id": "module-3/physics-and-worlds",
      "title": "Building the World",
      "description": "Before we spawn our robot, we need a world for it to live in. In Gazebo, the world is defined using an SDF file.",
      "sidebar": "tutorialSidebar"
    },
    "module-3/simulating-sensors": {
      "id": "module-3/simulating-sensors",
      "title": "Virtual Eyes & Ears (Sensors)",
      "description": "A simulation is only useful if the robot can perceive it. If we want to test our navigation algorithms, our simulated robot needs a simulated Lidar.",
      "sidebar": "tutorialSidebar"
    },
    "module-3/unity-visualization": {
      "id": "module-3/unity-visualization",
      "title": "The Pretty Face (Unity Visualization)",
      "description": "Gazebo is a scientific tool. Its graphics are \"good enough\" for debugging, but they aren't beautiful.",
      "sidebar": "tutorialSidebar"
    },
    "module-3/urdf-vs-sdf": {
      "id": "module-3/urdf-vs-sdf",
      "title": "The Format War (URDF vs. SDF)",
      "description": "As a roboticist, you are caught in a format war between two giants: ROS and Gazebo.",
      "sidebar": "tutorialSidebar"
    },
    "module-4/isaac-ros-gems": {
      "id": "module-4/isaac-ros-gems",
      "title": "Supercharging ROS (Isaac ROS GEMs)",
      "description": "Standard ROS 2 nodes often struggle with high-bandwidth data like 4K video streams or point clouds because they process data on the CPU. Isaac ROS GEMs are a collection of GPU-accelerated packages that replace these slow standard nodes.",
      "sidebar": "tutorialSidebar"
    },
    "module-4/isaac-sim-omniverse": {
      "id": "module-4/isaac-sim-omniverse",
      "title": "The Omniverse (Isaac Sim)",
      "description": "Welcome to the cutting edge of robotics simulation. In previous modules, we used Gazebo, which relies primarily on your CPU for physics calculations. Now, we are stepping into NVIDIA Isaac Sim, a photo-realistic, physically accurate virtual environment powered by NVIDIA Omniverse.",
      "sidebar": "tutorialSidebar"
    },
    "module-4/nav2-and-vslam": {
      "id": "module-4/nav2-and-vslam",
      "title": "Spatial Awareness (VSLAM & Nav2)",
      "description": "For a humanoid robot to be useful, it must answer two questions: \"Where am I?\" and \"How do I get there?\" In this section, we replace traditional Lidar-based SLAM with Visual SLAM using Isaac ROS.",
      "sidebar": "tutorialSidebar"
    },
    "module-4/rl-and-sim-to-real": {
      "id": "module-4/rl-and-sim-to-real",
      "title": "Walking with AI (Reinforcement Learning)",
      "description": "Classical robotics uses explicit mathematical equations (Inverse Kinematics) to calculate how a robot should move. However, for complex tasks like humanoid walking, the math becomes incredibly difficult.",
      "sidebar": "tutorialSidebar"
    },
    "module-4/synthetic-data-generation": {
      "id": "module-4/synthetic-data-generation",
      "title": "Training the Brain (Synthetic Data)",
      "description": "Training modern AI models (like Object Detection or Segmentation) requires massive datasets—often tens of thousands of labeled images. Collecting this data in the real world is slow, expensive, and sometimes dangerous.",
      "sidebar": "tutorialSidebar"
    },
    "module-5/bipedal-locomotion": {
      "id": "module-5/bipedal-locomotion",
      "title": "The Art of Falling (Bipedal Walking)",
      "description": "Walking is a complex feat of engineering. For a humanoid, walking is essentially a state of controlled falling. Every step is a move to catch the body before it hits the ground.",
      "sidebar": "tutorialSidebar"
    },
    "module-5/capstone-integration": {
      "id": "module-5/capstone-integration",
      "title": "Assembling the Avatar (The Capstone)",
      "description": "In this final section of Module 5, we look at how to integrate every technology we have learned so far into a single, cohesive \"Robot Brain.\" This is the goal of the Physical AI course: creating an autonomous humanoid avatar.",
      "sidebar": "tutorialSidebar"
    },
    "module-5/human-robot-interaction": {
      "id": "module-5/human-robot-interaction",
      "title": "Social Robotics (HRI)",
      "description": "Humanoid robots are designed to operate in human environments. This means they must follow social \"rules\" that we often take for granted. This field is called Human-Robot Interaction (HRI).",
      "sidebar": "tutorialSidebar"
    },
    "module-5/kinematics-dynamics": {
      "id": "module-5/kinematics-dynamics",
      "title": "The Math of Movement (Kinematics)",
      "description": "To make a humanoid robot move, we need to understand how its joints relate to the position of its body in 3D space. This study of motion without considering forces is called Kinematics.",
      "sidebar": "tutorialSidebar"
    },
    "module-5/manipulation-grasping": {
      "id": "module-5/manipulation-grasping",
      "title": "Grasping the World (MoveIt 2)",
      "description": "While the legs handle locomotion, the arms handle Manipulation. In ROS 2, the gold standard for controlling robotic arms is MoveIt 2.",
      "sidebar": "tutorialSidebar"
    },
    "module-6/future-ethics": {
      "id": "module-6/future-ethics",
      "title": "Living with Robots (Ethics & Future)",
      "description": "As we conclude this course, we must address the reality of releasing autonomous agents into the real world. A robot is not just a machine; it is a physical entity that shares space with humans.",
      "sidebar": "tutorialSidebar"
    },
    "module-6/hearing-with-whisper": {
      "id": "module-6/hearing-with-whisper",
      "title": "The Robot's Ears (Speech Recognition)",
      "description": "The first step in interaction is hearing. We need to convert sound waves (analog air pressure) into text (digital strings). For this, we use OpenAI Whisper, a state-of-the-art Automatic Speech Recognition (ASR) system.",
      "sidebar": "tutorialSidebar"
    },
    "module-6/intro-conversational-robotics": {
      "id": "module-6/intro-conversational-robotics",
      "title": "The Voice Interface (VUI)",
      "description": "For decades, we have controlled robots with keyboards, gamepads, and touchscreens. But if humanoids are to integrate seamlessly into our homes, they must speak our language. This is the Voice User Interface (VUI).",
      "sidebar": "tutorialSidebar"
    },
    "module-6/multimodal-interaction": {
      "id": "module-6/multimodal-interaction",
      "title": "Vision-Language-Action (VLA)",
      "description": "A voice assistant like Alexa is blind. It can hear \"Turn on the lights,\" but it cannot understand \"Pick up that red cup.\" For physical interaction, we need Multimodal AI—combining Vision and Language.",
      "sidebar": "tutorialSidebar"
    },
    "module-6/thinking-with-llms": {
      "id": "module-6/thinking-with-llms",
      "title": "The Cognitive Core (GPT Integration)",
      "description": "Now that the robot can hear \"Move forward,\" how does it know how to move? It needs a brain to translate natural language into robot instructions. We use a Large Language Model (LLM) like GPT-4.",
      "sidebar": "tutorialSidebar"
    }
  }
}